import pyodbc

# Assuming you already have a connection setup to BigQuery using pyodbc:
conn_str = 'your_connection_string_here'  # This needs to be properly set up
connection = pyodbc.connect(conn_str)

# Utility function to check if a table exists
def table_exists(connection, table_name):
    # Here, the `dataset_id` corresponds to your BigQuery dataset.
    sql = f"""SELECT COUNT(1) 
              FROM {dataset_id}.INFORMATION_SCHEMA.TABLES 
              WHERE table_name = '{table_name}'"""

    cursor = connection.cursor()
    cursor.execute(sql)
    result = cursor.fetchone()
    if result[0] > 0:
        return True
    else:
        return False

# Your existing code for setting the q_scope starts here
@callback(
    Output("scope-storage", "data"),   
    Output("scope-set-alert", 'is_open'),
    Output('loading-scope-output', 'children'),
    Output('scope-status', "value"),
    Input("scope-set-button", "n_clicks"),
    State("q-date-range", "value"),
    State("join-file-upload-toggle", "value"),
    State("scope-storage", "data")
)
def run_q_process(n_clicks, q_date_range, join_toggle_value, memory_scope):
    interactions = ctx.triggered_id

    # Your existing logic continues here...
    
    # If join_toggle_value contains 'join' (i.e., the switch is ON), and the uploaded DF exists, join the data
    if 'join' in join_toggle_value and table_exists(connection, 'file_upload'):  
        file_upload_join = "INNER JOIN file_upload F ON S.r_id = F.r_id"
    else:
        file_upload_join = ""

    # ... and the rest of your code
